{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='google-bert/bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "#加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载并处理数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9600/9600 [00:01<00:00, 5946.80 examples/s]\n",
      "Map: 100%|██████████| 1200/1200 [00:00<00:00, 5988.01 examples/s]\n",
      "Map: 100%|██████████| 1200/1200 [00:00<00:00, 5934.80 examples/s]\n",
      "Filter: 100%|██████████| 9600/9600 [00:00<00:00, 61404.86 examples/s]\n",
      "Filter: 100%|██████████| 1200/1200 [00:00<00:00, 46307.52 examples/s]\n",
      "Filter: 100%|██████████| 1200/1200 [00:00<00:00, 46319.88 examples/s]\n",
      "Map: 100%|██████████| 9286/9286 [00:00<00:00, 24157.51 examples/s]\n",
      "Map: 100%|██████████| 1158/1158 [00:00<00:00, 22601.65 examples/s]\n",
      "Map: 100%|██████████| 1157/1157 [00:00<00:00, 22762.09 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['input_ids', 'attention_mask', 'label'],\n",
       "         num_rows: 9286\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['input_ids', 'attention_mask', 'label'],\n",
       "         num_rows: 1158\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['input_ids', 'attention_mask', 'label'],\n",
       "         num_rows: 1157\n",
       "     })\n",
       " }),\n",
       " {'input_ids': tensor([ 101, 6848, 2885, 4403, 3736, 5709, 1736, 4638, 1333, 1728, 2218, 3221,\n",
       "          3175,  912, 8024,  103, 4510, 1220, 2820, 3461, 4684, 2970, 1168, 6809,\n",
       "          3862, 6804, 8024, 1453, 1741,  102]),\n",
       "  'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1]),\n",
       "  'label': tensor(3300)})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#加载数据集\n",
    "dataset = load_dataset(path='lansinuote/ChnSentiCorp')\n",
    "\n",
    "#编码\n",
    "f = lambda x: tokenizer(\n",
    "    x['text'], truncation=True, max_length=30, return_token_type_ids=False)\n",
    "dataset = dataset.map(f, remove_columns=['text', 'label'])\n",
    "\n",
    "#过滤句子长度\n",
    "f = lambda x: len(x['input_ids']) >= 30\n",
    "dataset = dataset.filter(f)\n",
    "\n",
    "\n",
    "#重置label字段\n",
    "def f(data):\n",
    "    #定义第15个字为label\n",
    "    data['label'] = data['input_ids'][15]\n",
    "\n",
    "    #替换句子中的第15个字为mask\n",
    "    data['input_ids'][15] = tokenizer.mask_token_id\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "dataset = dataset.map(f)\n",
    "\n",
    "#设置数据类型\n",
    "dataset.set_format('pt')\n",
    "\n",
    "dataset, dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([8, 30])\n",
      "attention_mask torch.Size([8, 30])\n",
      "label torch.Size([8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1160"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = torch.utils.data.DataLoader(dataset['train'],\n",
    "                                     batch_size=8,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "data = next(iter(loader))\n",
    "\n",
    "for k, v in data.items():\n",
    "    print(k, v.shape)\n",
    "\n",
    "len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 配 置 一 流 ， 在 我 [UNK] 拷 贝 五 分 钟 的 [MASK] 件 ， 这 台 机 只 要 30 秒 就 搞 掂 ， [SEP]\n",
      "文\n",
      "==============\n",
      "[CLS] 如 果 对 于 日 本 这 个 国 家 仍 仅 仅 停 [MASK] 在 书 本 知 识 的 话 ， 那 么 这 本 书 [SEP]\n",
      "留\n",
      "==============\n",
      "[CLS] 一 直 都 想 给 儿 子 买 国 内 的 科 普 读 [MASK] ， 但 是 都 没 有 买 ， 原 因 是 说 教 [SEP]\n",
      "物\n",
      "==============\n",
      "[CLS] 服 务 很 热 情 ， 房 间 温 度 正 好 ， 隔 [MASK] 效 果 好 ， 地 理 位 置 优 越 ， 性 价 [SEP]\n",
      "音\n",
      "==============\n",
      "[CLS] 从 开 始 读 关 于 家 庭 教 育 的 书 ， 到 [MASK] 在 也 不 知 读 了 多 少 本 。 但 至 今 [SEP]\n",
      "现\n",
      "==============\n",
      "[CLS] 偶 觉 得 这 句 真 的 好 有 道 理 。 道 出 [MASK] 我 的 心 声 啊 ！ [UNK] 出 租 车 就 这 样 [SEP]\n",
      "了\n",
      "==============\n",
      "[CLS] 散 热 有 些 差 ， 可 能 [UNK] 的 通 病 。 右 [MASK] 面 有 时 热 的 太 过 分 。 价 格 降 的 [SEP]\n",
      "下\n",
      "==============\n",
      "[CLS] 坐 [UNK] 时 问 司 机 这 家 酒 店 怎 么 样 \" [MASK] 老 酒 店, 人 气 旺, 服 务 好, 众 [SEP]\n",
      ",\n",
      "==============\n"
     ]
    }
   ],
   "source": [
    "#查看数据样例\n",
    "for q, a in zip(data['input_ids'], data['label']):\n",
    "    print(tokenizer.decode(q))\n",
    "    print(tokenizer.decode(a))\n",
    "    print('==============')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义下游任务模型（Downstream Tasks by Pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(9.9584, grad_fn=<NllLossBackward0>),\n",
       " tensor([[6.3790e-05, 4.9582e-05, 3.0027e-05,  ..., 4.5211e-05, 1.3296e-05,\n",
       "          7.4145e-05],\n",
       "         [6.3213e-05, 4.2645e-05, 3.7453e-05,  ..., 2.7083e-05, 1.8871e-05,\n",
       "          4.3168e-05],\n",
       "         [8.2669e-05, 5.7174e-05, 3.6212e-05,  ..., 2.2250e-05, 3.4547e-05,\n",
       "          4.0065e-05],\n",
       "         ...,\n",
       "         [7.7786e-05, 5.1327e-05, 5.1337e-05,  ..., 2.4219e-05, 3.5704e-05,\n",
       "          2.8052e-05],\n",
       "         [5.6067e-05, 3.0498e-05, 3.0617e-05,  ..., 2.9279e-05, 2.0529e-05,\n",
       "          6.7594e-05],\n",
       "         [5.4029e-05, 6.1294e-05, 4.0449e-05,  ..., 1.9086e-05, 2.3124e-05,\n",
       "          3.6273e-05]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定义模型\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #加载预训练模型\n",
    "        from transformers import AutoModel\n",
    "        self.pretrained = AutoModel.from_pretrained(\n",
    "            'google-bert/bert-base-chinese')\n",
    "\n",
    "        self.fc = torch.nn.Linear(in_features=768,\n",
    "                                  out_features=tokenizer.vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, label=None):\n",
    "        #使用预训练模型抽取数据特征\n",
    "        with torch.no_grad():\n",
    "            last_hidden_state = self.pretrained(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask).last_hidden_state\n",
    "\n",
    "        #取第15个词的特征向量\n",
    "        last_hidden_state = last_hidden_state[:, 15]\n",
    "\n",
    "        #对抽取的特征只取第一个字的结果做分类即可\n",
    "        out = self.fc(last_hidden_state).softmax(dim=1)\n",
    "\n",
    "        #计算loss\n",
    "        loss = None\n",
    "        if label is not None:\n",
    "            loss = torch.nn.functional.cross_entropy(out, label)\n",
    "\n",
    "        return loss, out\n",
    "\n",
    "\n",
    "model = Model()\n",
    "\n",
    "model(**data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 执行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 1160 9.958338737487793 0.0\n",
      "0 200 1160 9.95831298828125 0.0\n",
      "0 400 1160 9.873355865478516 0.25\n",
      "0 600 1160 9.574552536010742 0.5\n",
      "0 800 1160 9.816194534301758 0.25\n",
      "0 1000 1160 9.471392631530762 0.5\n",
      "1 0 1160 9.954161643981934 0.0\n",
      "1 200 1160 9.823026657104492 0.125\n",
      "1 400 1160 9.803387641906738 0.25\n",
      "1 600 1160 9.49328327178955 0.625\n",
      "1 800 1160 9.527310371398926 0.5\n",
      "1 1000 1160 9.695521354675293 0.25\n",
      "2 0 1160 9.363061904907227 0.625\n",
      "2 200 1160 9.483329772949219 0.625\n",
      "2 400 1160 9.556771278381348 0.375\n",
      "2 600 1160 9.676772117614746 0.25\n",
      "2 800 1160 9.473091125488281 0.5\n",
      "2 1000 1160 9.459622383117676 0.5\n",
      "3 0 1160 9.854050636291504 0.125\n",
      "3 200 1160 9.470598220825195 0.5\n",
      "3 400 1160 9.407782554626465 0.75\n",
      "3 600 1160 9.48289966583252 0.625\n",
      "3 800 1160 9.481666564941406 0.5\n",
      "3 1000 1160 9.61184310913086 0.375\n",
      "4 0 1160 9.560114860534668 0.5\n",
      "4 200 1160 9.721332550048828 0.25\n",
      "4 400 1160 9.378260612487793 0.625\n",
      "4 600 1160 9.243274688720703 0.75\n",
      "4 800 1160 9.71529769897461 0.25\n",
      "4 1000 1160 9.716795921325684 0.25\n"
     ]
    }
   ],
   "source": [
    "#执行训练\n",
    "def train():\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    for epoch in range(5):\n",
    "        for i, data in enumerate(loader):\n",
    "            loss, out = model(**data)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if i % 200 == 0:\n",
    "                out = out.argmax(dim=1)\n",
    "                acc = (out == data['label']).sum().item() / len(data['label'])\n",
    "                print(epoch, i, len(loader), loss.item(), acc)\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 执行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 144 0.5\n",
      "1 144 0.4375\n",
      "2 144 0.375\n",
      "3 144 0.40625\n",
      "4 144 0.4\n",
      "5 144 0.4166666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4166666666666667"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#执行测试\n",
    "def test():\n",
    "    loader_test = torch.utils.data.DataLoader(dataset['test'],\n",
    "                                              batch_size=8,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(loader_test):\n",
    "        with torch.no_grad():\n",
    "            _, out = model(**data)\n",
    "\n",
    "        out = out.argmax(dim=1)\n",
    "        correct += (out == data['label']).sum().item()\n",
    "        total += len(data['label'])\n",
    "\n",
    "        print(i, len(loader_test), correct / total)\n",
    "\n",
    "        if i == 5:\n",
    "            break\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlplearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
